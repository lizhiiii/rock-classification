{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20f595d9-343a-4366-ae30-46799534214b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda | GPU: NVIDIA L20\n"
     ]
    }
   ],
   "source": [
    "# rock_hier_train_v2.py  ── ConvNeXtV2-Large + 双头层次分类 (改进版)\n",
    "# ==============================================================\n",
    "# 1. 标准库 & 基础设置\n",
    "# --------------------------------------------------------------\n",
    "import os, math, time, random, warnings, pathlib\n",
    "import platform, psutil, csv\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch_ema import ExponentialMovingAverage\n",
    "import timm\n",
    "\n",
    "import csv\n",
    "import glob\n",
    "import math\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "import platform\n",
    "import psutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch_ema import ExponentialMovingAverage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 数据处理\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 第三方SOTA模型/工具包\n",
    "import timm  # 顶级SOTA模型库，实际好像服务器翻不了墙\n",
    "\n",
    "# PyTorch及生态\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# 可视化\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# 混合精度训练提速省显存grad防止半精度数值不稳定，滑动平均能提供泛化能力\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch_ema import ExponentialMovingAverage\n",
    "\n",
    "# 进度条\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "SEED = 114514\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.benchmark, torch.backends.cudnn.deterministic = True, False\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {DEVICE} | GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72d8a94c-00e1-41c2-93fb-ca9dbccc4e0e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 2. 数据路径 & 超参数\n",
    "# --------------------------------------------------------------\n",
    "DATA_ROOT   = \"/shareddata/project/dataset\"\n",
    "TRAIN_CSV   = f\"{DATA_ROOT}/train_val/train_labels.csv\"\n",
    "VAL_CSV     = f\"{DATA_ROOT}/train_val/val_labels.csv\"\n",
    "IMG_DIR_T   = f\"{DATA_ROOT}/train_val/train\"\n",
    "IMG_DIR_V   = f\"{DATA_ROOT}/train_val/val\"\n",
    "TEST_IMGDIR = f\"{DATA_ROOT}/test/test_images\"\n",
    "TEST_CSV    = f\"{DATA_ROOT}/test/test_ids.csv\"\n",
    "\n",
    "NUM_PARENT  = 3\n",
    "BATCH_SIZE  = 64 #占用显存43977MiB /  46068MiB\n",
    "NUM_EPOCHS  = 60 #早停无所谓        \n",
    "MIXUP_CUTMIX_STOP = int(NUM_EPOCHS * 0.8)   # 后 20% epoch 关闭 MixUp/CutMix\n",
    "BASE_LR     = 2e-4 \n",
    "WARM_EPOCHS = 5\n",
    "WEIGHT_DECAY= 1e-4\n",
    "MAX_GRAD_NORM = 1.0\n",
    "EMA_DECAY  = 0.9999\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a0344a9-f324-465a-a09b-e674e282c42f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: train=102213, val=15000, child=19160\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# 3. 数据增强 & Dataset\n",
    "# --------------------------------------------------------------\n",
    "import torchvision.transforms as T\n",
    "MEAN = [0.46798, 0.45764, 0.44035]\n",
    "STD  = [0.18461, 0.18712, 0.19482]\n",
    "\n",
    "tf_train = T.Compose([\n",
    "    T.RandomResizedCrop(224),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandAugment(num_ops=2, magnitude=8),  # 略降幅度\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(MEAN, STD),\n",
    "])\n",
    "tf_eval = T.Compose([\n",
    "    T.Resize(256), T.CenterCrop(224),\n",
    "    T.ToTensor(),  T.Normalize(MEAN, STD)\n",
    "])\n",
    "\n",
    "class RockDataset(Dataset):\n",
    "    def __init__(self, csv_path, img_dir, transform):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        self.paths = df[\"id\"].values\n",
    "        self.p_labels = df[\"label\"].values.astype(np.int64)\n",
    "        self.c_labels = pd.factorize(df[\"sublabel\"])[0].astype(np.int64)\n",
    "        self.transform = transform\n",
    "        self.img_dir = img_dir\n",
    "    def __len__(self): return len(self.paths)\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(os.path.join(self.img_dir, self.paths[idx])).convert(\"RGB\")\n",
    "        return self.transform(img), self.p_labels[idx], self.c_labels[idx]\n",
    "\n",
    "train_ds, val_ds = RockDataset(TRAIN_CSV, IMG_DIR_T, tf_train), RockDataset(VAL_CSV, IMG_DIR_V, tf_eval)\n",
    "NUM_CHILD = int(train_ds.c_labels.max()) + 1\n",
    "print(f\"Dataset: train={len(train_ds)}, val={len(val_ds)}, child={NUM_CHILD}\")\n",
    "\n",
    "train_dl = DataLoader(train_ds, BATCH_SIZE, shuffle=True,  num_workers=8, pin_memory=True)\n",
    "val_dl   = DataLoader(val_ds,   BATCH_SIZE, shuffle=False, num_workers=8, pin_memory=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "141547dc-6140-4e99-ab84-b59599fe6280",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 4. 模型定义 (ConvNeXtV2-Large 主干 + 双头)\n",
    "# --------------------------------------------------------------\n",
    "CKPT_PATH  = \"convnextv2_large.fcmae_ft_in22k_in1k_384.bin\"\n",
    "MODEL_NAME = \"convnextv2_large.fcmae_ft_in22k_in1k_384\"\n",
    "\n",
    "state_dict = torch.load(CKPT_PATH, map_location=\"cpu\")\n",
    "state_dict = {k: v for k, v in state_dict.items() if not k.startswith(\"head.\")}\n",
    "backbone   = timm.create_model(MODEL_NAME, pretrained=False, num_classes=0)\n",
    "backbone.load_state_dict(state_dict, strict=False)\n",
    "feat_dim   = backbone.num_features\n",
    "\n",
    "head_parent = nn.Linear(feat_dim, NUM_PARENT)\n",
    "head_child  = nn.Linear(feat_dim, NUM_CHILD)\n",
    "\n",
    "backbone, head_parent, head_child = backbone.to(DEVICE), head_parent.to(DEVICE), head_child.to(DEVICE)\n",
    "\n",
    "# 封装 forward\n",
    "def forward_features(x):\n",
    "    return backbone(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d700267-54ba-4340-8f96-e6164d179571",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 5. 损失函数 (类别加权 CE) & 优化器 & 调度器\n",
    "# --------------------------------------------------------------\n",
    "# 5.1 类别权重 (父类)\n",
    "cnt_parent = Counter(train_ds.p_labels.tolist())\n",
    "tot_samples = len(train_ds)\n",
    "w_parent = torch.tensor([tot_samples / cnt_parent[i] for i in range(NUM_PARENT)], dtype=torch.float).to(DEVICE)\n",
    "criterion_parent = nn.CrossEntropyLoss(weight=w_parent, label_smoothing=0.1)\n",
    "\n",
    "# 5.2 子类可选加权 (实际没使用)\n",
    "criterion_child  = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# 5.3 Optimizer (分组 lr)\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {\"params\": backbone.parameters(),   \"lr\": BASE_LR * 0.2},\n",
    "    {\"params\": head_parent.parameters(),\"lr\": BASE_LR},\n",
    "    {\"params\": head_child.parameters(), \"lr\": BASE_LR}\n",
    "], weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# 5.4 Scheduler: Linear Warmup -> Cosine\n",
    "def lr_lambda(cur_epoch):\n",
    "    if cur_epoch < WARM_EPOCHS:\n",
    "        return float(cur_epoch + 1) / WARM_EPOCHS\n",
    "    return 0.5 * (1 + math.cos(math.pi * (cur_epoch - WARM_EPOCHS) / (NUM_EPOCHS - WARM_EPOCHS)))\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "scaler = GradScaler()\n",
    "ema    = ExponentialMovingAverage([*backbone.parameters(), *head_parent.parameters(), *head_child.parameters()], decay=EMA_DECAY)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db820305-cf2a-4c71-ba28-5e97ca4af2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 6. MixUp / CutMix 工具\n",
    "# --------------------------------------------------------------\n",
    "def rand_bbox(size, lam):\n",
    "    H, W = size[2], size[3]\n",
    "    cut_rat = math.sqrt(1. - lam)\n",
    "    cut_w, cut_h = int(W * cut_rat), int(H * cut_rat)\n",
    "    cx, cy = np.random.randint(W), np.random.randint(H)\n",
    "    x1, y1 = np.clip(cx - cut_w // 2, 0, W), np.clip(cy - cut_h // 2, 0, H)\n",
    "    x2, y2 = np.clip(cx + cut_w // 2, 0, W), np.clip(cy + cut_h // 2, 0, H)\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "def mixup_cutmix(images, labels, alpha=0.2):\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    bs   = images.size(0)\n",
    "    idx  = torch.randperm(bs, device=images.device)\n",
    "    if random.random() < 0.5:  # MixUp\n",
    "        mixed = lam * images + (1 - lam) * images[idx]\n",
    "    else:                      # CutMix\n",
    "        x1,y1,x2,y2 = rand_bbox(images.size(), lam)\n",
    "        mixed = images.clone()\n",
    "        mixed[:, :, y1:y2, x1:x2] = images[idx, :, y1:y2, x1:x2]\n",
    "        lam = 1 - ((x2-x1)*(y2-y1)/(images.size(-1)*images.size(-2)))\n",
    "    return mixed, labels, labels[idx], lam\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcc1492a-d520-4023-bcdb-c77f635f1c7e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 7. 评估函数 (使用 EMA 权重)\n",
    "# --------------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def evaluate():\n",
    "    backbone.eval(); head_parent.eval(); head_child.eval()\n",
    "    totals, corrects, loss_sum = 0, 0, 0.0\n",
    "    for imgs, p_lbl, _ in val_dl:\n",
    "        imgs, p_lbl = imgs.to(DEVICE), p_lbl.to(DEVICE)\n",
    "        with autocast():\n",
    "            feats = forward_features(imgs)\n",
    "            logits = head_parent(feats)\n",
    "            loss   = criterion_parent(logits, p_lbl)\n",
    "        loss_sum += loss.item() * imgs.size(0)\n",
    "        corrects += (logits.argmax(1) == p_lbl).sum().item()\n",
    "        totals   += imgs.size(0)\n",
    "    return loss_sum / totals, corrects / totals\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c542d0d4-79f7-4d95-abc7-69271207c861",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[1/60]: 100%|██████████████████████████████████████████████████████████████████| 1598/1598 [17:57<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E01] lr=8.00e-06 | train=0.9540 | p[loss=0.8023, acc=69.30%] | c[loss=10.0139, acc=0.01%] | time=1164.7s\n",
      "↳ Parent Confusion Matrix:\n",
      " [[3504  643  853]\n",
      " [ 917 3565  518]\n",
      " [1201  473 3326]]\n",
      "  ✅ 最强父类模型已更新\n",
      "  ✅ 最强子类模型已更新\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[2/60]: 100%|██████████████████████████████████████████████████████████████████| 1598/1598 [17:47<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E02] lr=8.00e-06 | train=0.8902 | p[loss=0.7573, acc=72.65%] | c[loss=10.0196, acc=0.02%] | time=1153.5s\n",
      "↳ Parent Confusion Matrix:\n",
      " [[3576  659  765]\n",
      " [ 769 3825  406]\n",
      " [1003  500 3497]]\n",
      "  ✅ 最强父类模型已更新\n",
      "  ✅ 最强子类模型已更新\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[3/60]: 100%|██████████████████████████████████████████████████████████████████| 1598/1598 [17:48<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E03] lr=8.00e-06 | train=0.8672 | p[loss=0.7311, acc=74.37%] | c[loss=10.0224, acc=0.01%] | time=1154.0s\n",
      "↳ Parent Confusion Matrix:\n",
      " [[3631  614  755]\n",
      " [ 693 3910  397]\n",
      " [ 929  457 3614]]\n",
      "  ✅ 最强父类模型已更新\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[4/60]: 100%|██████████████████████████████████████████████████████████████████| 1598/1598 [17:48<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E04] lr=8.00e-06 | train=0.8491 | p[loss=0.7112, acc=75.57%] | c[loss=10.0247, acc=0.01%] | time=1154.1s\n",
      "↳ Parent Confusion Matrix:\n",
      " [[3697  600  703]\n",
      " [ 648 3965  387]\n",
      " [ 910  416 3674]]\n",
      "  ✅ 最强父类模型已更新\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[5/60]: 100%|██████████████████████████████████████████████████████████████████| 1598/1598 [17:46<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E05] lr=8.00e-06 | train=0.8263 | p[loss=0.6974, acc=76.46%] | c[loss=10.0251, acc=0.01%] | time=1148.9s\n",
      "↳ Parent Confusion Matrix:\n",
      " [[3740  591  669]\n",
      " [ 615 4030  355]\n",
      " [ 902  399 3699]]\n",
      "  ✅ 最强父类模型已更新\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[6/60]: 100%|██████████████████████████████████████████████████████████████████| 1598/1598 [17:46<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E06] lr=1.60e-05 | train=0.8176 | p[loss=0.6884, acc=77.30%] | c[loss=10.4327, acc=0.09%] | time=1149.4s\n",
      "↳ Parent Confusion Matrix:\n",
      " [[3775  557  668]\n",
      " [ 607 4040  353]\n",
      " [ 833  387 3780]]\n",
      "  ✅ 最强父类模型已更新\n",
      "  ✅ 最强子类模型已更新\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[7/60]: 100%|██████████████████████████████████████████████████████████████████| 1598/1598 [17:46<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E07] lr=2.40e-05 | train=0.8178 | p[loss=0.6787, acc=77.87%] | c[loss=10.6438, acc=0.09%] | time=1149.0s\n",
      "↳ Parent Confusion Matrix:\n",
      " [[3837  514  649]\n",
      " [ 609 4040  351]\n",
      " [ 854  343 3803]]\n",
      "  ✅ 最强父类模型已更新\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[8/60]: 100%|██████████████████████████████████████████████████████████████████| 1598/1598 [17:46<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E08] lr=3.20e-05 | train=0.8133 | p[loss=0.6687, acc=78.59%] | c[loss=10.7988, acc=0.05%] | time=1148.2s\n",
      "↳ Parent Confusion Matrix:\n",
      " [[3890  467  643]\n",
      " [ 599 4039  362]\n",
      " [ 823  318 3859]]\n",
      "  ✅ 最强父类模型已更新\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[9/60]: 100%|██████████████████████████████████████████████████████████████████| 1598/1598 [17:46<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E09] lr=4.00e-05 | train=0.8036 | p[loss=0.6586, acc=79.52%] | c[loss=10.9657, acc=0.05%] | time=1148.5s\n",
      "↳ Parent Confusion Matrix:\n",
      " [[3904  489  607]\n",
      " [ 564 4098  338]\n",
      " [ 768  306 3926]]\n",
      "  ✅ 最强父类模型已更新\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[10/60]: 100%|█████████████████████████████████████████████████████████████████| 1598/1598 [17:46<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E10] lr=4.00e-05 | train=0.7966 | p[loss=0.6525, acc=79.93%] | c[loss=11.1043, acc=0.01%] | time=1148.4s\n",
      "↳ Parent Confusion Matrix:\n",
      " [[3942  456  602]\n",
      " [ 561 4098  341]\n",
      " [ 761  290 3949]]\n",
      "  ✅ 最强父类模型已更新\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[11/60]: 100%|█████████████████████████████████████████████████████████████████| 1598/1598 [17:46<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E11] lr=4.00e-05 | train=0.7835 | p[loss=0.6458, acc=80.49%] | c[loss=11.2092, acc=0.03%] | time=1148.6s\n",
      "↳ Parent Confusion Matrix:\n",
      " [[3941  460  599]\n",
      " [ 535 4132  333]\n",
      " [ 713  287 4000]]\n",
      "  ✅ 最强父类模型已更新\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[12/60]: 100%|█████████████████████████████████████████████████████████████████| 1598/1598 [17:46<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E12] lr=3.99e-05 | train=0.7553 | p[loss=0.6408, acc=80.71%] | c[loss=11.3061, acc=0.03%] | time=1148.2s\n",
      "↳ Parent Confusion Matrix:\n",
      " [[3940  462  598]\n",
      " [ 522 4143  335]\n",
      " [ 699  278 4023]]\n",
      "  ✅ 最强父类模型已更新\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[13/60]: 100%|█████████████████████████████████████████████████████████████████| 1598/1598 [17:46<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E13] lr=3.97e-05 | train=0.7320 | p[loss=0.6374, acc=81.05%] | c[loss=11.3469, acc=0.03%] | time=1149.1s\n",
      "↳ Parent Confusion Matrix:\n",
      " [[3919  476  605]\n",
      " [ 488 4196  316]\n",
      " [ 675  282 4043]]\n",
      "  ✅ 最强父类模型已更新\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[14/60]: 100%|█████████████████████████████████████████████████████████████████| 1598/1598 [17:46<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E14] lr=3.95e-05 | train=0.7093 | p[loss=0.6384, acc=81.12%] | c[loss=11.3650, acc=0.01%] | time=1148.8s\n",
      "↳ Parent Confusion Matrix:\n",
      " [[3912  456  632]\n",
      " [ 494 4188  318]\n",
      " [ 663  269 4068]]\n",
      "  ✅ 最强父类模型已更新\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[15/60]: 100%|█████████████████████████████████████████████████████████████████| 1598/1598 [17:45<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E15] lr=3.92e-05 | train=0.7020 | p[loss=0.6401, acc=81.47%] | c[loss=11.4073, acc=0.02%] | time=1147.8s\n",
      "↳ Parent Confusion Matrix:\n",
      " [[3910  443  647]\n",
      " [ 468 4222  310]\n",
      " [ 638  274 4088]]\n",
      "  ✅ 最强父类模型已更新\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[16/60]: 100%|█████████████████████████████████████████████████████████████████| 1598/1598 [17:46<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E16] lr=3.88e-05 | train=0.6798 | p[loss=0.6430, acc=81.71%] | c[loss=11.4206, acc=0.03%] | time=1148.3s\n",
      "↳ Parent Confusion Matrix:\n",
      " [[3906  441  653]\n",
      " [ 466 4233  301]\n",
      " [ 616  266 4118]]\n",
      "  ✅ 最强父类模型已更新\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[17/60]: 100%|█████████████████████████████████████████████████████████████████| 1598/1598 [17:46<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E17] lr=3.84e-05 | train=0.6692 | p[loss=0.6463, acc=81.53%] | c[loss=11.4367, acc=0.03%] | time=1148.6s\n",
      "↳ Parent Confusion Matrix:\n",
      " [[3882  461  657]\n",
      " [ 461 4224  315]\n",
      " [ 614  262 4124]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[18/60]: 100%|█████████████████████████████████████████████████████████████████| 1598/1598 [17:46<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E18] lr=3.79e-05 | train=0.6578 | p[loss=0.6483, acc=81.72%] | c[loss=11.4240, acc=0.03%] | time=1148.7s\n",
      "↳ Parent Confusion Matrix:\n",
      " [[3886  439  675]\n",
      " [ 445 4238  317]\n",
      " [ 603  263 4134]]\n",
      "  ✅ 最强父类模型已更新\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[19/60]: 100%|█████████████████████████████████████████████████████████████████| 1598/1598 [17:46<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E19] lr=3.74e-05 | train=0.6484 | p[loss=0.6516, acc=81.55%] | c[loss=11.4160, acc=0.03%] | time=1149.3s\n",
      "↳ Parent Confusion Matrix:\n",
      " [[3857  452  691]\n",
      " [ 446 4233  321]\n",
      " [ 589  268 4143]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[20/60]: 100%|█████████████████████████████████████████████████████████████████| 1598/1598 [17:45<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E20] lr=3.68e-05 | train=0.6377 | p[loss=0.6534, acc=81.61%] | c[loss=11.4204, acc=0.03%] | time=1148.1s\n",
      "↳ Parent Confusion Matrix:\n",
      " [[3844  445  711]\n",
      " [ 430 4241  329]\n",
      " [ 588  256 4156]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[21/60]: 100%|█████████████████████████████████████████████████████████████████| 1598/1598 [17:46<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E21] lr=3.62e-05 | train=0.6245 | p[loss=0.6549, acc=81.77%] | c[loss=11.4018, acc=0.03%] | time=1148.5s\n",
      "↳ Parent Confusion Matrix:\n",
      " [[3828  449  723]\n",
      " [ 421 4246  333]\n",
      " [ 560  248 4192]]\n",
      "  ✅ 最强父类模型已更新\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[22/60]: 100%|█████████████████████████████████████████████████████████████████| 1598/1598 [17:46<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E22] lr=3.55e-05 | train=0.6249 | p[loss=0.6567, acc=81.85%] | c[loss=11.3920, acc=0.03%] | time=1148.5s\n",
      "↳ Parent Confusion Matrix:\n",
      " [[3835  444  721]\n",
      " [ 419 4243  338]\n",
      " [ 550  251 4199]]\n",
      "  ✅ 最强父类模型已更新\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[23/60]: 100%|█████████████████████████████████████████████████████████████████| 1598/1598 [17:46<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E23] lr=3.47e-05 | train=0.6192 | p[loss=0.6580, acc=81.79%] | c[loss=11.3926, acc=0.03%] | time=1147.9s\n",
      "↳ Parent Confusion Matrix:\n",
      " [[3832  445  723]\n",
      " [ 425 4235  340]\n",
      " [ 548  250 4202]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[24/60]:  20%|████████████▉                                                     | 313/1598 [03:30<14:15,  1.50it/s]"
     ]
    }
   ],
   "source": [
    "# ==== 5. Train / Eval with Logging, EMA, EarlyStopping, Resume ======================\n",
    "best_p_acc, best_c_acc = 0.0, 0.0\n",
    "patience, non_improve = 5, 0\n",
    "\n",
    "CKPT_DIR = \"checkpoint\"\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "start_epoch = 1\n",
    "\n",
    "# === Resume from checkpoint ===\n",
    "ckpt_list = sorted(glob.glob(os.path.join(CKPT_DIR, \"best_parent_*.pth\")), key=os.path.getmtime)\n",
    "if ckpt_list:\n",
    "    latest = ckpt_list[-1]\n",
    "    print(f\"🔄 Resumed from {latest}\")\n",
    "    ckpt = torch.load(latest, map_location=DEVICE)\n",
    "    backbone.load_state_dict(ckpt[\"backbone\"])\n",
    "    head_parent.load_state_dict(ckpt[\"head_p\"])\n",
    "    if \"head_c\" in ckpt: head_child.load_state_dict(ckpt[\"head_c\"])\n",
    "    if \"opt\" in ckpt: optimizer.load_state_dict(ckpt[\"opt\"])\n",
    "    if \"sched\" in ckpt: scheduler.load_state_dict(ckpt[\"sched\"])\n",
    "    if \"scaler\" in ckpt: scaler.load_state_dict(ckpt[\"scaler\"])\n",
    "    if \"ema\" in ckpt: ema.load_state_dict(ckpt[\"ema\"])\n",
    "    best_p_acc = ckpt.get(\"p_acc\", best_p_acc)\n",
    "    best_c_acc = ckpt.get(\"c_acc\", best_c_acc)\n",
    "    start_epoch = ckpt.get(\"epoch\", start_epoch) + 1\n",
    "    print(f\"  → start_epoch={start_epoch} | best_p_acc={best_p_acc:.4f} | best_c_acc={best_c_acc:.4f}\")\n",
    "\n",
    "# === CSV Logger ===\n",
    "log_file = pathlib.Path(\"train_log.csv\")\n",
    "with log_file.open(\"a\", newline=\"\") as log_fh:\n",
    "    logger = csv.writer(log_fh)\n",
    "    if log_file.stat().st_size == 0:\n",
    "        logger.writerow([\"epoch\",\"lr\",\"train_loss\",\"p_loss\",\"p_acc\",\"c_loss\",\"c_acc\",\"time_sec\"])\n",
    "\n",
    "    def evaluate():\n",
    "        backbone.eval(); head_parent.eval(); head_child.eval()\n",
    "        p_correct = c_correct = total = 0\n",
    "        p_loss_sum = c_loss_sum = 0.0\n",
    "        all_p_preds, all_p_gts = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for imgs, y_p, y_c in val_dl:\n",
    "                imgs, y_p, y_c = imgs.to(DEVICE), y_p.to(DEVICE), y_c.to(DEVICE)\n",
    "                with autocast():\n",
    "                    feats = forward_features(imgs)\n",
    "                    logits_p = head_parent(feats)\n",
    "                    logits_c = head_child(feats)\n",
    "                    loss_p = criterion_parent(logits_p, y_p)\n",
    "                    loss_c = criterion_child(logits_c, y_c)\n",
    "\n",
    "                p_loss_sum += loss_p.item() * y_p.size(0)\n",
    "                c_loss_sum += loss_c.item() * y_c.size(0)\n",
    "                pred_p = logits_p.argmax(1)\n",
    "                p_correct += (pred_p == y_p).sum().item()\n",
    "                c_correct += (logits_c.argmax(1) == y_c).sum().item()\n",
    "                total += y_p.size(0)\n",
    "\n",
    "                all_p_preds.extend(pred_p.cpu().tolist())\n",
    "                all_p_gts.extend(y_p.cpu().tolist())\n",
    "\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        cm = confusion_matrix(all_p_gts, all_p_preds, labels=[0,1,2])\n",
    "        return p_loss_sum/total, p_correct/total, c_loss_sum/total, c_correct/total, cm\n",
    "\n",
    "    # === Training Loop ===\n",
    "    for epoch in range(start_epoch, NUM_EPOCHS + 1):\n",
    "        t0 = time.time()\n",
    "        backbone.train(); head_parent.train(); head_child.train()\n",
    "        run_loss, tot = 0.0, 0\n",
    "\n",
    "        use_mix = epoch <= MIXUP_CUTMIX_STOP\n",
    "        for imgs, y_p, y_c in tqdm(train_dl, desc=f\"Epoch[{epoch}/{NUM_EPOCHS}]\", ncols=120):\n",
    "            imgs, y_p, y_c = imgs.to(DEVICE), y_p.to(DEVICE), y_c.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if use_mix:\n",
    "                imgs, y_p_a, y_p_b, lam = mixup_cutmix(imgs, y_p)\n",
    "\n",
    "            with autocast():\n",
    "                feats = forward_features(imgs)\n",
    "                logits_p = head_parent(feats)\n",
    "                logits_c = head_child(feats)\n",
    "\n",
    "                loss_p = (lam * criterion_parent(logits_p, y_p_a) + (1 - lam) * criterion_parent(logits_p, y_p_b)) if use_mix else criterion_parent(logits_p, y_p)\n",
    "                child_weight = max(0.0, (epoch - WARM_EPOCHS) / (NUM_EPOCHS - WARM_EPOCHS))\n",
    "                loss_c = criterion_child(logits_c, y_c)\n",
    "                loss = (1 - child_weight) * loss_p + child_weight * loss_c\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(backbone.parameters(), MAX_GRAD_NORM)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            ema.update()\n",
    "\n",
    "            run_loss += loss_p.item() * imgs.size(0)\n",
    "            tot += imgs.size(0)\n",
    "\n",
    "        if epoch > WARM_EPOCHS:\n",
    "            scheduler.step()\n",
    "\n",
    "        # === EMA 验证 ===\n",
    "        with ema.average_parameters():\n",
    "            p_val_loss, p_val_acc, c_val_loss, c_val_acc, cm = evaluate()\n",
    "\n",
    "        train_loss = run_loss / tot\n",
    "        elapsed = time.time() - t0\n",
    "        cur_lr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "        print(f\"[E{epoch:02d}] lr={cur_lr:.2e} | train={train_loss:.4f} | p[loss={p_val_loss:.4f}, acc={p_val_acc*100:.2f}%] | c[loss={c_val_loss:.4f}, acc={c_val_acc*100:.2f}%] | time={elapsed:.1f}s\")\n",
    "        print(\"↳ Parent Confusion Matrix:\\n\", cm)\n",
    "\n",
    "        logger.writerow([epoch, f\"{cur_lr:.2e}\", f\"{train_loss:.4f}\", f\"{p_val_loss:.4f}\", f\"{p_val_acc:.4f}\", f\"{c_val_loss:.4f}\", f\"{c_val_acc:.4f}\", f\"{elapsed:.1f}\"])\n",
    "        log_fh.flush()\n",
    "\n",
    "        improved = False\n",
    "        if p_val_acc > best_p_acc:\n",
    "            best_p_acc, improved = p_val_acc, True\n",
    "            torch.save({\n",
    "                \"backbone\": backbone.state_dict(),\n",
    "                \"head_p\": head_parent.state_dict(),\n",
    "                \"head_c\": head_child.state_dict(),\n",
    "                \"opt\": optimizer.state_dict(),\n",
    "                \"sched\": scheduler.state_dict(),\n",
    "                \"scaler\": scaler.state_dict(),\n",
    "                \"ema\": ema.state_dict(),\n",
    "                \"p_acc\": best_p_acc,\n",
    "                \"c_acc\": c_val_acc,\n",
    "                \"epoch\": epoch\n",
    "            }, os.path.join(CKPT_DIR, f\"best_parent_{best_p_acc:.4f}.pth\"))\n",
    "            print(\"  ✅ 最强父类模型已更新\")\n",
    "        if c_val_acc > best_c_acc:\n",
    "            best_c_acc = c_val_acc\n",
    "            torch.save({\n",
    "                \"backbone\": backbone.state_dict(),\n",
    "                \"head_c\": head_child.state_dict(),\n",
    "                \"c_acc\": best_c_acc\n",
    "            }, os.path.join(CKPT_DIR, f\"best_child_{best_c_acc:.4f}.pth\"))\n",
    "            print(\"  ✅ 最强子类模型已更新\")\n",
    "\n",
    "        non_improve = 0 if improved else non_improve + 1\n",
    "        if non_improve >= patience:\n",
    "            print(f\"⏹️ Early stopping at epoch {epoch} after {patience} non-improving epochs.\")\n",
    "            break\n",
    "\n",
    "print(f\"✅ Training Finished | Best Parent Acc={best_p_acc:.4f} | Best Child Acc={best_c_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0da564dd-23fb-49e5-a15d-338b48030271",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==> Generating submission.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████████████████████████████████████████████████████████████████| 235/235 [20:49<00:00,  5.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved submission.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# 9. 推理生成 submission.csv (EMA 权重 + 多尺度水平翻转 TTA)\n",
    "# --------------------------------------------------------------\n",
    "print(\"\\n==> Generating submission.csv\")\n",
    "CKPT_DIR = \"checkpoint\"\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "#state = torch.load(\"best_parent_0.8185.pth\", map_location=DEVICE)\n",
    "best_ckpt = sorted(os.listdir(CKPT_DIR))[-1]\n",
    "state = torch.load(os.path.join(CKPT_DIR, best_ckpt), map_location=DEVICE)\n",
    "\n",
    "backbone.load_state_dict(state[\"backbone\"]); head_parent.load_state_dict(state[\"head_p\"])\n",
    "ema.load_state_dict(state[\"ema\"])\n",
    "backbone.eval(); head_parent.eval()\n",
    "\n",
    "# TTA 变换 (3 尺度 × 左右翻转)\n",
    "SCALES = [224, 256, 288]\n",
    "tta_trans = []\n",
    "for s in SCALES:\n",
    "    for flip in [False, True]:\n",
    "        t = [T.Resize(s), T.CenterCrop(224)]\n",
    "        if flip: t.append(T.RandomHorizontalFlip(p=1.0))\n",
    "        t += [T.ToTensor(), T.Normalize(MEAN, STD)]\n",
    "        tta_trans.append(T.Compose(t))\n",
    "TTA_N = len(tta_trans)\n",
    "\n",
    "df_test = pd.read_csv(TEST_CSV)\n",
    "ids = df_test[\"id\"].values\n",
    "out_ids, out_labels = [], []\n",
    "\n",
    "with torch.no_grad(), ema.average_parameters(), autocast():\n",
    "    for i in tqdm(range(0, len(ids), BATCH_SIZE), ncols=120, desc=\"Inference\"):\n",
    "        batch_ids = ids[i:i+BATCH_SIZE]\n",
    "        imgs_all = []\n",
    "        for fname in batch_ids:\n",
    "            img = Image.open(os.path.join(TEST_IMGDIR, fname)).convert(\"RGB\")\n",
    "            imgs_all.extend([tf(img) for tf in tta_trans])\n",
    "        imgs_all = torch.stack(imgs_all).to(DEVICE)\n",
    "\n",
    "        feats = forward_features(imgs_all)\n",
    "        logits = head_parent(feats).view(len(batch_ids), TTA_N, NUM_PARENT)\n",
    "        logits = logits.mean(1)          # TTA 平均\n",
    "        preds  = logits.argmax(1).cpu().numpy()\n",
    "\n",
    "        out_ids.extend(batch_ids)\n",
    "        out_labels.extend(preds.tolist())\n",
    "\n",
    "pd.DataFrame({\"id\": out_ids, \"label\": out_labels}).to_csv(\"submission.csv\", index=False)\n",
    "print(\"Saved submission.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
